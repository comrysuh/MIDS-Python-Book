{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a238aebe-7d41-4e2c-bc45-ea55783d2fa4",
   "metadata": {
    "id": "cfbdd729"
   },
   "source": [
    "##### Exercise 14.1 - Remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fcc51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "718fcc51",
    "outputId": "c2c39c62-9a2b-4911-9c3c-332e97aa49ce"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.1 - Remove missing values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Tim', 'Mary', 'David', None],\n",
    "    'English Score': [59, np.nan, None, 72, 81],\n",
    "    'Maths Score': [85, 67, 90, 88, 95]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print('Original DataFrame:\\n',df)\n",
    "print('The number of missing values in dataset df:', df.isnull().sum().sum())\n",
    "\n",
    "df1 = df.drop(columns=['English Score'])\n",
    "print('Smaller dataset df1:\\n',df1)\n",
    "print('The number of missing values in dataset df1:', df1.isnull().sum().sum())\n",
    "\n",
    "df_row_drop = df.dropna()\n",
    "print('\\nDataFrame after dropping rows with any missing values:\\n', df_row_drop)\n",
    "\n",
    "df_col_drop = df.dropna(axis=1)\n",
    "print('\\nDataFrame after dropping columns with any missing values:\\n', df_col_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472911b8-33f2-412d-a3ef-8ea7ed084998",
   "metadata": {},
   "source": [
    "#### Exercise 14.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K89cViZAatkJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K89cViZAatkJ",
    "outputId": "bf3db27d-f682-4459-f73b-52ea0f47d8c7"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Name': ['Alice', 'Tim', 'Mary', 'David', None, 'David', 'Alice'],\n",
    "    'English Score': [59, np.nan, None, 72, 81, 72, 59],\n",
    "    'Maths Score': [85, 67, 90, 88, 95, None, 85]}\n",
    "\n",
    "df2 = pd.DataFrame(data)\n",
    "print('Original DataFrame:\\n',df2)\n",
    "print('Original DataFrame shape:',df2.shape)\n",
    "print('The number of missing values in dataset df2:', df2.isnull().sum().sum())\n",
    "\n",
    "df3 = df2.drop_duplicates(subset=['Name'])\n",
    "print('\\n New DataFrame:\\n',df3)\n",
    "print('New DataFrame shape:',df3.shape)\n",
    "print('The number of missing values in dataset df3:', df3.isnull().sum().sum())\n",
    "\n",
    "df4 = df2.drop_duplicates()\n",
    "print('\\n Next New DataFrame:\\n',df4)\n",
    "print('Next New DataFrame shape:',df4.shape)\n",
    "print('The number of missing values in dataset df4:', df4.isnull().sum().sum())\n",
    "\n",
    "print('\\ndf2 original columns:', df2.columns)\n",
    "print('As list:', df2.columns.tolist())\n",
    "print('With extra column name:', df2.columns.tolist() + ['Physics Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327633c0-e4dd-4f48-b1ec-ab07434c6ea3",
   "metadata": {
    "id": "0d6891df"
   },
   "source": [
    "#### Exercise 14.3 - Load data and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e0dc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be3e0dc3",
    "outputId": "88086b14-e128-460e-bc2c-e6a64aa7aaf0"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.3 - Load data and preprocess data\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "breast_cancer = load_breast_cancer()\n",
    "df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "\n",
    "X = df.drop(columns=['mean radius'])\n",
    "\n",
    "y_class = breast_cancer.target\n",
    "y_reg = df['mean radius']\n",
    "\n",
    "print(\"Data matrix df shape:\", df.shape)\n",
    "print(\"Data matrix X shape:\", X.shape)\n",
    "print(\"Classification target shape:\", y_class.shape)\n",
    "print(\"Regression target shape:\", y_reg.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412fb2c",
   "metadata": {
    "id": "b412fb2c"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.3 - continued\n",
    "df_new = X.copy()\n",
    "df_new['y_class'] = y_class\n",
    "df_new['y_reg'] = y_reg\n",
    "print('Shape of df_new now is:',df_new.shape)\n",
    "\n",
    "print('The number of missing values in dataset:', df_new.isnull().sum().sum())\n",
    "df_new = df_new.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78adf7ff",
   "metadata": {
    "id": "78adf7ff"
   },
   "source": [
    "#### Drop duplicates based on features and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cca26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d6cca26",
    "outputId": "0be94bcb-2b36-4985-fb13-1e1d05db8509"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.3 - continued\n",
    "drdup_class = df_new.drop_duplicates(\n",
    "    subset=X.columns.tolist() + ['y_class']\n",
    ")\n",
    "print('Shape of drdup_class is: ',drdup_class.shape)\n",
    "print('The number of missing values in dataset drdup_class:', drdup_class.isnull().sum().sum())\n",
    "\n",
    "drdup_reg = df_new.drop_duplicates(\n",
    "    subset=X.columns.tolist() + ['y_reg']\n",
    ")\n",
    "print('Shape of drdup_reg is: ',drdup_reg.shape)\n",
    "print('The number of missing values in dataset drdup_reg:', drdup_reg.isnull().sum().sum())\n",
    "\n",
    "drdup_both = pd.merge(drdup_class, drdup_reg, how='inner')\n",
    "print('The number of missing values in dataset drdup_both:', drdup_both.isnull().sum().sum())\n",
    "print(f'After removing duplications (per target), the shape is: {drdup_both.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeade95-d894-4ab5-b802-6dc6ccc3adde",
   "metadata": {
    "id": "42a5e054"
   },
   "source": [
    "#### Exercise 14.4 -  Remove inconsistent rows including same features but different targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a693b8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a693b8f",
    "outputId": "f4124df6-413e-4830-9d01-e72dc6a57ba8"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.4 - Remove inconsistent rows including same features but different targets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Length': [59, 43, 59, 72, 81],\n",
    "    'Width': [85, 67, 85, 88, 95],\n",
    "    'Class':[1, 1, 0, 0, 0]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print('Original DataFrame:\\n', df)\n",
    "\n",
    "#Class is the target column so remove it\n",
    "toy_X = df.drop(columns=['Class'])\n",
    "print('Data Frame with target class removed:\\n',toy_X)\n",
    "\n",
    "# Group rows based on having the same numbers in them\n",
    "grouped = df.groupby(toy_X.columns.tolist())\n",
    "\n",
    "inconsistent_indices = []\n",
    "\n",
    "# If rows in the same group have different values in 'Class' then they are inconsistent\n",
    "for _, group in grouped:\n",
    "    if group['Class'].nunique() > 1:\n",
    "        inconsistent_indices.extend(group.index.tolist())\n",
    "\n",
    "# Print the indices of the inconsistent rows\n",
    "print('Indices of any inconsistent rows',inconsistent_indices)\n",
    "print(f'Found {len(inconsistent_indices)} inconsistent rows.')\n",
    "\n",
    "df_cleaned = df.drop(index=inconsistent_indices).reset_index(drop=True)\n",
    "print('Final cleaned data shape:', df_cleaned.shape)\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc8398b-05ee-41b2-ace7-275b1aa56b79",
   "metadata": {
    "id": "630c1d95"
   },
   "source": [
    "#### Exercise 14.5 - Split data into the training and the test set - listings needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f79106",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31f79106",
    "outputId": "0a88cc6a-43e9-4a69-fcbd-c656b839967e"
   },
   "outputs": [],
   "source": [
    "#Listing 14.5 - Listings needed\n",
    "\n",
    "#From Listing 14.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "#From Listing 14.2 - Load data and preprocess data\n",
    "breast_cancer = load_breast_cancer()\n",
    "print(breast_cancer.keys())\n",
    "\n",
    "df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "print('Breast cancer data frame shape: ',df.shape)\n",
    "\n",
    "X = df.drop(columns=['mean radius'])\n",
    "y_class = breast_cancer.target\n",
    "y_reg = df['mean radius']\n",
    "\n",
    "print(\"Data matrix shape:\", X.shape)\n",
    "\n",
    "#From Listing 14.6 - Split data into the training and the test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X, y_class, y_reg, test_size=0.2, random_state=421)\n",
    "\n",
    "X_Strain, X_val, y_class_Strain, y_class_val, y_reg_Strain, y_reg_val = train_test_split(\n",
    "    X_train, y_class_train, y_reg_train, test_size=0.2, random_state=421)\n",
    "\n",
    "print('The size of the smaller training set:\\n', X_Strain.shape)\n",
    "print('The size of the validation set:\\n', X_val.shape)\n",
    "print('The size of the test set:\\n', X_test.shape)\n",
    "print('The smaller training set class distribution:\\n', Counter(y_class_Strain))\n",
    "print('The validation set class distribution:\\n', Counter(y_class_val))\n",
    "print('The test set class distribution:\\n', Counter(y_class_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce7570-f00f-4f5a-bdf0-654837c95c31",
   "metadata": {
    "id": "30be9c16"
   },
   "source": [
    "#### Exercise 14.5 - Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48f8b6",
   "metadata": {
    "id": "eb48f8b6"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.5 - amended Listing 14.7 to print out the values required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_highly_correlated_features(df, threshold):\n",
    "    df = df.copy()\n",
    "    removed_features = []\n",
    "\n",
    "    while True:\n",
    "        corr_matrix = df.corr().abs()\n",
    "        print('\\n Original correlation matrix:\\n',corr_matrix)\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        print('Upper correlation matrix:\\n',upper)\n",
    "        max_corr = upper.max().max()\n",
    "        print('max_corr:', max_corr)\n",
    "        if max_corr < threshold:\n",
    "            print(f'All correlations below threshold {threshold}.')\n",
    "            break\n",
    "\n",
    "        A, B = upper.stack().idxmax()\n",
    "        print('Features with max correlation:',A,B)\n",
    "        avg_corr_A = corr_matrix[A].drop(index=A).mean()\n",
    "        avg_corr_B = corr_matrix[B].drop(index=B).mean()\n",
    "        print(f'Average correlation with other features for {A} is: {avg_corr_A} ')\n",
    "        print(f'Average correlation with other features for {B} is: {avg_corr_B}')\n",
    "\n",
    "        to_remove = A if avg_corr_A > avg_corr_B else B\n",
    "        print('Therfore remove:',to_remove)\n",
    "        df.drop(columns=to_remove, inplace=True)\n",
    "        removed_features.append(to_remove)\n",
    "\n",
    "    return df, removed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85420e7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85420e7e",
    "outputId": "22a825a0-c6b0-4ee3-e26a-7901fae86906"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.5 - test code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(421)\n",
    "size = 50\n",
    "\n",
    "data = {\n",
    "    'A': np.random.rand(size),\n",
    "    'B': np.random.rand(size),\n",
    "    'C': np.random.rand(size),\n",
    "    'D': np.random.rand(size) * 0.5,  }\n",
    "\n",
    "data['E'] = data['B'] * 0.95 + np.random.rand(size) * 0.01\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X_reduced, removed_features = remove_highly_correlated_features(df, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd410b-1bf5-4fe9-9bc6-7eade80e74fc",
   "metadata": {
    "id": "ba37c4bd"
   },
   "source": [
    "#### Exercise 14.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804323c5-d2e0-4d5f-bc56-4d4006a3a3c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5eec0ca",
    "outputId": "5beb635b-4870-417d-9ee0-893cfb309038"
   },
   "outputs": [],
   "source": [
    "#Question 14.6 - imports needed = Listing 14.1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDRegressor, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load data = Listing 14.2\n",
    "breast_cancer = load_breast_cancer()\n",
    "print(breast_cancer.keys())\n",
    "\n",
    "df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "\n",
    "X = df.drop(columns=['mean radius'])\n",
    "y_class = breast_cancer.target\n",
    "y_reg = df['mean radius']\n",
    "\n",
    "print(\"Data matrix shape:\", X.shape)\n",
    "\n",
    "#Split data into the training and the test set = Listing 14.6\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(\n",
    "    X, y_class, y_reg, test_size=0.2, random_state=421)\n",
    "\n",
    "X_Strain, X_val, y_class_Strain, y_class_val, y_reg_Strain, y_reg_val = train_test_split(\n",
    "    X_train, y_class_train, y_reg_train, test_size=0.2, random_state=421)\n",
    "\n",
    "print('The size of the smaller training set:\\n', X_Strain.shape)\n",
    "print('The size of the validation set:\\n', X_val.shape)\n",
    "print('The size of the test set:\\n', X_test.shape)\n",
    "print('The smaller training set class distribution:\\n', Counter(y_class_Strain))\n",
    "print('The validation set class distribution:\\n', Counter(y_class_val))\n",
    "print('The test set class distribution:\\n', Counter(y_class_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tGG2Z7GNLXJf",
   "metadata": {
    "id": "tGG2Z7GNLXJf"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.6 - remove highly correlated features using Listing 14.7\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_highly_correlated_features(df, threshold):\n",
    "    df = df.copy()\n",
    "    removed_features = []\n",
    "\n",
    "    while True:\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "        max_corr = upper.max().max()\n",
    "        if max_corr < threshold:\n",
    "            print(f'All correlations below threshold {threshold}.')\n",
    "            break\n",
    "\n",
    "        A, B = upper.stack().idxmax()\n",
    "\n",
    "        avg_corr_A = corr_matrix[A].drop(index=A).mean()\n",
    "        avg_corr_B = corr_matrix[B].drop(index=B).mean()\n",
    "\n",
    "        to_remove = A if avg_corr_A > avg_corr_B else B\n",
    "        df.drop(columns=to_remove, inplace=True)\n",
    "        removed_features.append(to_remove)\n",
    "\n",
    "    return df, removed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf84c9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caf84c9d",
    "outputId": "324ce47b-b2e2-476e-d74c-bb8d8757cb8d"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.6 - Feature selection = Listing 14.8\n",
    "X_Strain_reduced, removed_features = remove_highly_correlated_features(X_Strain, threshold=0.50)\n",
    "kept_features= X_Strain_reduced.columns.tolist()\n",
    "X_train_reduced = X_train[kept_features]\n",
    "X_val_reduced = X_val[kept_features]\n",
    "X_test_reduced = X_test[kept_features]\n",
    "print(f'Kept features: {kept_features}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f4bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "3d7f4bfd",
    "outputId": "a35a6e35-c540-478d-e6fa-6d584fb3b1d7"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.6 - PCA - first dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA().fit(X_train_scaled)\n",
    "proj_X_train = pca.transform(X_train_scaled)\n",
    "proj_X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('The first two PCs capture the variance in percentage:',pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1])\n",
    "print('The first six PCs capture the variance in percentage:',np.sum(pca.explained_variance_ratio_[0:6]))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(np.arange(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "axes[0].set_xlabel('Principal Component', fontsize=16)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=16)\n",
    "\n",
    "axes[1].scatter(proj_X_train[y_class_train == 0, 0], proj_X_train[y_class_train == 0, 1], label='Malignant', color='r')\n",
    "axes[1].scatter(proj_X_train[y_class_train == 1, 0], proj_X_train[y_class_train == 1, 1], label='Benign', color='b')\n",
    "axes[1].set_xlabel('PC1', fontsize=16)\n",
    "axes[1].set_ylabel('PC2', fontsize=16)\n",
    "axes[1].set_xlim([-7, 18])\n",
    "axes[1].set_ylim([-10, 15])\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].scatter(proj_X_test[y_class_test == 0, 0], proj_X_test[y_class_test == 0, 1], label='Malignant', color='r')\n",
    "axes[2].scatter(proj_X_test[y_class_test == 1, 0], proj_X_test[y_class_test == 1, 1], label='Benign', color='b')\n",
    "axes[2].set_xlabel('PC1', fontsize=16)\n",
    "axes[2].set_ylabel('PC2', fontsize=16)\n",
    "axes[2].set_xlim([-7, 18])\n",
    "axes[2].set_ylim([-10, 15])\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"pca_full.eps\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1248d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "ba1248d7",
    "outputId": "093321a0-7680-41d2-8ea9-e04bbfebf782"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.6 - PCA - second dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_reduced)\n",
    "X_train_scaled = scaler.transform(X_train_reduced)\n",
    "X_test_scaled = scaler.transform(X_test_reduced)\n",
    "X_train_scaled = scaler.transform(X_train_reduced)\n",
    "\n",
    "pca = PCA().fit(X_train_scaled)\n",
    "proj_X_train = pca.transform(X_train_scaled)\n",
    "proj_X_test = pca.transform(X_test_scaled)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print('The first two PCs capture the variance in percentage:',pca.explained_variance_ratio_[0] + pca.explained_variance_ratio_[1])\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].plot(np.arange(1, len(explained_variance) + 1), explained_variance, marker='o')\n",
    "axes[0].set_xlabel('Principal Component', fontsize=16)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=16)\n",
    "\n",
    "axes[1].scatter(proj_X_train[y_class_train == 0, 0], proj_X_train[y_class_train == 0, 1], label='Malignant', color='r')\n",
    "axes[1].scatter(proj_X_train[y_class_train == 1, 0], proj_X_train[y_class_train == 1, 1], label='Benign', color='b')\n",
    "axes[1].set_xlabel('PC1', fontsize=16)\n",
    "axes[1].set_ylabel('PC2', fontsize=16)\n",
    "axes[1].set_xlim([-4, 10])\n",
    "axes[1].set_ylim([-5, 5])\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].scatter(proj_X_test[y_class_test == 0, 0], proj_X_test[y_class_test == 0, 1], label='Malignant', color='r')\n",
    "axes[2].scatter(proj_X_test[y_class_test == 1, 0], proj_X_test[y_class_test == 1, 1], label='Benign', color='b')\n",
    "axes[2].set_xlabel('PC1', fontsize=16)\n",
    "axes[2].set_ylabel('PC2', fontsize=16)\n",
    "axes[2].set_xlim([-4, 10])\n",
    "axes[2].set_ylim([-5, 5])\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"pca_reduced.eps\", dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3a19c5-5b57-47fe-a356-1bfeaad1e99d",
   "metadata": {
    "id": "ceeadf13"
   },
   "source": [
    "#### Exercise 14.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fba0c67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fba0c67",
    "outputId": "60d5fe6f-d492-440a-b713-a7c6e6d29cbc"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.7 - Re-run Listing 14.12 - namely, the Ridge Regression with the best alpha\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "best_alpha = 0.215443\n",
    "final_model = Ridge(alpha=best_alpha)\n",
    "final_model.fit(X_train_scaled, y_reg_train)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "mse_test = mean_squared_error(y_reg_test, y_test_pred)\n",
    "r2_test = r2_score(y_reg_test, y_test_pred)\n",
    "\n",
    "print(f'Final test MSE (Ridge, alpha={best_alpha:.4f}): {mse_test:.6f}')\n",
    "print(f'Final test coefficient of determination score : {r2_test:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51702c53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51702c53",
    "outputId": "0793e072-d4ee-4ccf-8d57-1d7cda658b4b"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.7 - Part 1 - Identify outliers\n",
    "residuals = y_reg_test - y_test_pred\n",
    "residual_std = np.std(residuals)\n",
    "\n",
    "outlier_mask = np.abs(residuals) > 2 * residual_std\n",
    "outlier_indices = outlier_mask[outlier_mask].index\n",
    "outlier_pos_indices = np.where(outlier_mask)[0]  \n",
    "print('Outlier actual indices:', outlier_indices)\n",
    "print(\"Outlier positional indices:\", outlier_pos_indices)\n",
    "print(f'Identified {outlier_mask.sum()} outliers out of {len(y_reg_test)} test samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045199cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "045199cf",
    "outputId": "8418974d-9d05-44df-e2f3-ea34405d000f"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.7 Part 2 - Remove outliers and evaluate the original model on the cleaned test set\n",
    "non_outlier_mask = ~outlier_mask\n",
    "X_test_clean = X_test_scaled[non_outlier_mask]\n",
    "y_test_clean = y_reg_test[non_outlier_mask]\n",
    "\n",
    "y_test_clean_pred = final_model.predict(X_test_clean)\n",
    "\n",
    "mse_clean = mean_squared_error(y_test_clean, y_test_clean_pred)\n",
    "r2_clean = r2_score(y_test_clean, y_test_clean_pred)\n",
    "\n",
    "print('\\n--- Test performance after removing outliers ---')\n",
    "print(f'MSE (clean test set): {mse_clean:.6f}')\n",
    "print(f'Coefficient of determination (clean test set): {r2_clean:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452858da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "452858da",
    "outputId": "99afa7fb-151e-4936-bffd-673ff4eb163e"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.7 - Part 3 - Retrain ridge with augmented training set including test outliers\n",
    "X_outliers = X_test_scaled[outlier_mask]\n",
    "y_outliers = y_reg_test[outlier_mask]\n",
    "\n",
    "X_train_augmented = np.vstack([X_train_scaled, X_outliers])\n",
    "y_train_augmented = np.concatenate([y_reg_train, y_outliers])\n",
    "\n",
    "X_train_new, X_val_new, y_train_new, y_val_new = train_test_split(\n",
    "    X_train_augmented, y_train_augmented, test_size=0.2, random_state=421)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_new)\n",
    "X_Strain_scaled = scaler.transform(X_train_new)\n",
    "X_val_scaled = scaler.transform(X_val_new)\n",
    "\n",
    "alphas = np.logspace(-2, 2, 25)\n",
    "val_errors = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_Strain_scaled, y_train_new)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    mse = mean_squared_error(y_val_new, y_val_pred)\n",
    "    val_errors.append(mse)\n",
    "\n",
    "best_alpha_augmented = alphas[np.argmin(val_errors)]\n",
    "print(f'Best alpha: {best_alpha_augmented:.6f}')\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_augmented)\n",
    "X_train_aug_scaled = scaler.transform(X_train_augmented)\n",
    "X_test_clean_scaled = scaler.transform(X_test_clean)\n",
    "\n",
    "augmented_model = Ridge(alpha=best_alpha_augmented)\n",
    "augmented_model.fit(X_train_aug_scaled, y_train_augmented)\n",
    "\n",
    "y_test_aug_pred = augmented_model.predict(X_test_clean_scaled)\n",
    "mse_aug = mean_squared_error(y_test_clean, y_test_aug_pred)\n",
    "r2_aug = r2_score(y_test_clean, y_test_aug_pred)\n",
    "\n",
    "print('\\n--- The fitted model ---')\n",
    "print('Coefficients:\\n', augmented_model.coef_)\n",
    "print('Intercept:', augmented_model.intercept_)\n",
    "print('Alpha:', augmented_model.alpha)\n",
    "\n",
    "print('\\n--- Test performance after retraining with outliers ---')\n",
    "print(f'MSE (Full test set): {mse_aug:.6f}')\n",
    "print(f'Coefficient of determination  (full test set): {r2_aug:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d245c1b3-c113-4ca4-9978-40ae65d7afed",
   "metadata": {
    "id": "ee9e4aa5"
   },
   "source": [
    "#### Exercise 14.8 - Change the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27eab2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f27eab2a",
    "outputId": "b2fe26f1-9842-495a-d2ac-8feb9a44d861",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exercise 14.8 - Change the threshold\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score\n",
    "\n",
    "np.random.seed(421)\n",
    "\n",
    "n_samples, n_features = 50, 2\n",
    "\n",
    "mean_0, cov_0 = [1, 2], [[1, 0.5], [0.5, 1]]\n",
    "mean_1, cov_1 = [3, 3], [[1, -0.3], [-0.3, 1]]\n",
    "\n",
    "data1 = np.random.multivariate_normal(mean_0, cov_0, n_samples)\n",
    "data2 = np.random.multivariate_normal(mean_1, cov_1, n_samples)\n",
    "X = np.vstack((data1, data2))\n",
    "y = np.hstack((np.zeros(n_samples), np.ones(n_samples)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "class_model = LogisticRegression(penalty=None, solver='lbfgs', tol=1e-8)\n",
    "class_model.fit(X_train_scaled, y_train)\n",
    "y_pred = class_model.predict(X_test_scaled)\n",
    "\n",
    "print('Sklearn logistic regression accuracy at threshold = 0.5 is:', accuracy_score(y_test, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "y_proba = class_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "thresholds = [0.3, 0.5, 0.7]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    print(f'\\n--- Threshold: {thresh} ---')\n",
    "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred_thresh)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap='Blues')\n",
    "\n",
    "    prec = precision_score(y_test, y_pred_thresh)\n",
    "    rec = recall_score(y_test, y_pred_thresh)\n",
    "    print(f'Precision: {prec:.3f}, Recall: {rec:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d09bf57-8a68-498c-9c9a-f10034588084",
   "metadata": {},
   "source": [
    "#### Exercise 14.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18716bf5-a893-4023-bcc0-79ace6d24492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 14.9 - Early stopping - part 1\n",
    "import copy\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1, learning_rate='constant', eta0=0.01, random_state=42, warm_start=True)\n",
    "\n",
    "n_epochs = 100\n",
    "patience = 5\n",
    "best_mse = float('inf')\n",
    "best_epoch = 0\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "\n",
    "scaler = StandardScaler().fit(X_Strain)\n",
    "X_Strain_scaled = scaler.transform(X_Strain)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    sgd.fit(X_Strain_scaled, y_reg_Strain)\n",
    "\n",
    "    y_train_pred = sgd.predict(X_Strain_scaled)\n",
    "    y_val_pred = sgd.predict(X_val_scaled)\n",
    "\n",
    "    mse_train = mean_squared_error(y_reg_Strain, y_train_pred)\n",
    "    mse_val = mean_squared_error(y_reg_val, y_val_pred)\n",
    "\n",
    "    train_errors.append(mse_train)\n",
    "    val_errors.append(mse_val)\n",
    "\n",
    "    if mse_val < best_mse - 1e-6:\n",
    "        best_mse = mse_val\n",
    "        best_epoch = epoch\n",
    "        best_model = copy.deepcopy(sgd)\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= patience:\n",
    "            print(f'Early stopping triggered at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "print(f'Best validation MSE: {best_mse:.4f} at epoch {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde3677a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "fde3677a",
    "outputId": "ba8c2537-f170-4d3e-e884-f94d29f2f332"
   },
   "outputs": [],
   "source": [
    "#Exercise 14.9 - Early stopping - part 2\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_errors, label='Train MSE')\n",
    "plt.plot(val_errors, label='Validation MSE')\n",
    "plt.axvline(best_epoch, linestyle='--', color='red', label='Best Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('Early Stopping: MSE vs Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BSSp2JlqkdWG",
   "metadata": {
    "id": "BSSp2JlqkdWG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
