{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634316c-013f-43eb-a435-8bfd7e0621d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Listing 9.1 - Linear Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ee911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.1 - Linear Activation Function\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 1])\n",
    "X = x[np.newaxis,:]\n",
    "print('The data is:', X)\n",
    "Target = np.array([[0.5, 0]])\n",
    "W = np.array([[0.5, 0.5], [0.25, 0.25]])\n",
    "\n",
    "Y0 = np.matmul(X, W.T)\n",
    "print('The predictions are:\\n', Y0)\n",
    "\n",
    "error0 = np.sum((Target - Y0)**2)/2\n",
    "print('The initial error is:', error0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c963111-b45e-4026-8640-427968f38696",
   "metadata": {},
   "source": [
    "#### Listing 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.2\n",
    "import sympy as sp\n",
    "\n",
    "d = sp.symbols('d')\n",
    "x = sp.MatrixSymbol('x', 1, d)\n",
    "w = sp.MatrixSymbol('w', 1, d)\n",
    "t = sp.MatrixSymbol('t', 1, 1)\n",
    "\n",
    "Q = (t-x*w.T).T * (t-x*w.T)/2\n",
    "Q.diff(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee366a-227a-4b96-bdf3-86ada940fb67",
   "metadata": {},
   "source": [
    "#### Listing 9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be309b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.3\n",
    "import sympy as sp\n",
    "\n",
    "x,w,t = sp.symbols('x w t')\n",
    "Q = (t-x*w) * (t-x*w)/2\n",
    "Q.diff(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3bb59a-2b7d-46a2-b707-bc42462b9932",
   "metadata": {},
   "source": [
    "#### Listing 9.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.4\n",
    "def update_weights_linear(data, target, weights, epsilon):    \n",
    "    pred = np.matmul(data, weights.T)\n",
    "    Res = target - pred\n",
    "    Qvalue = -np.matmul(Res.T, data)   \n",
    "    print('The derivatives are:\\n', Qvalue)\n",
    "    newweights = weights - epsilon * Qvalue\n",
    "\n",
    "    return newweights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd0fb5-ea30-4e54-a3b5-ab372df3ef6c",
   "metadata": {},
   "source": [
    "#### Listing 9.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245143df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.5\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1]])\n",
    "Target = np.array([[0.5, 0]])\n",
    "W = np.array([[0.5, 0.5], [0.25, 0.25]])\n",
    "epsilon = 0.1\n",
    "\n",
    "newW = update_weights_linear(X, Target, W, epsilon)\n",
    "print('The weights after one iteration are:\\n', newW)\n",
    "Y1 = np.matmul(X, newW.T)\n",
    "print('The predictions after one iteration are:\\n', Y1)\n",
    "error1 = np.sum((Target - Y1)**2)/2\n",
    "print('The error after one iteration is:', np.round(error1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d22d1",
   "metadata": {},
   "source": [
    "#### Example 2 - Logistic Signoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda667e5-a918-4f34-9863-d93ec9431079",
   "metadata": {},
   "source": [
    "#### Listing 9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5183d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.6\n",
    "def sigmoid(a):    \n",
    "    return 1/(np.exp(-a)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2399d-b811-4af3-8e0a-6d16dd385251",
   "metadata": {},
   "source": [
    "#### Listing 9.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b45e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.7\n",
    "import sympy as sp\n",
    "\n",
    "a = sp.symbols('a')\n",
    "ga = 1/(sp.exp(-a)+1)\n",
    "sp.diff(ga, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2a31a-0311-4325-8e42-2c6ee17c58fc",
   "metadata": {},
   "source": [
    "#### Listing 9.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c453a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.8\n",
    "import sympy as sp\n",
    "\n",
    "x, w, t, y= sp.symbols('x w t y')\n",
    "ga = 1/(sp.exp(-x*w)+1) \n",
    "Q = (t-y) * (t-y)/2\n",
    "\n",
    "Qexpr = Q.subs(y, ga)\n",
    "print('The error function is: \\n',Qexpr)\n",
    "sp.diff(Qexpr, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741da0a0-5e8e-4c6b-9801-dbccb0aea9ac",
   "metadata": {},
   "source": [
    "#### Listing 9.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.9\n",
    "def update_weights_sigmoid(data, target, weights, epsilon):  \n",
    "    A0 = np.matmul(data, weights.T)\n",
    "    pred = np.round(sigmoid(A0),3)\n",
    "    Res = target-pred\n",
    "    delta = -Res * (sigmoid(A0) * (1-sigmoid(A0)))\n",
    "    Qvalue = np.matmul(delta.T, data)   \n",
    "    print('The derivatives are:\\n', Qvalue)\n",
    "    newweights = weights - epsilon * Qvalue\n",
    "\n",
    "    return newweights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2adc9-287c-462f-84f7-58e88ee01886",
   "metadata": {},
   "source": [
    "#### Listing 9.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ca6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.10\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1]])\n",
    "Target = np.array([[0.5, 0]])\n",
    "W = np.array([[0.5, 0.5], [0.25, 0.25]])\n",
    "epsilon = 0.1\n",
    "\n",
    "A0 = np.matmul(X,W.T)\n",
    "Y0 = np.round(sigmoid(A0),3)\n",
    "init_error = np.sum((Target - Y0)**2)/2\n",
    "print('Initial error is:',np.round(init_error,3))\n",
    "\n",
    "newW = update_weights_sigmoid(X, Target, W, epsilon)\n",
    "print('The updated weights are:\\n', newW)\n",
    "A1 = np.matmul(X, newW.T)\n",
    "Y1 = np.round(sigmoid(A1),3)\n",
    "print('The predictions are:\\n', Y1)\n",
    "error = np.sum((Target - Y1)**2)/2\n",
    "print('The final error is:', np.round(error,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5d5208",
   "metadata": {},
   "source": [
    "#### Example 3 - A simple two-layer neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184afd14",
   "metadata": {},
   "source": [
    "###### The Feed-forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe5520-c655-453a-b2aa-7d9d58a44536",
   "metadata": {},
   "source": [
    "#### Listing 9.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.11\n",
    "def prediction(data, weights_1, weights_2):\n",
    "    A_1 = np.matmul(data, weights_1.T)\n",
    "    Z_1 = np.round(sigmoid(A_1), 3)\n",
    "    \n",
    "    A_2 = np.matmul(Z_1, weights_2.T)\n",
    "    Y = np.round(A_2, 3)\n",
    "    \n",
    "    return Y, A_2, Z_1, A_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c2497d",
   "metadata": {},
   "source": [
    "###### The error back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de52dd7-99b8-4fbc-b916-06e2587c4c96",
   "metadata": {},
   "source": [
    "#### Listing 9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faf43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.12\n",
    "def update_weights(data, target, weights_1, weights_2, epsilon):  \n",
    "    Y, A_2, Z_1, A_1 = prediction(data, weights_1, weights_2)\n",
    "    \n",
    "    delta_2 = target - Y\n",
    "    newweights_2 = weights_2 + epsilon * Z_1 * delta_2.T\n",
    "    print('delta_2 is:\\n', delta_2)\n",
    "   \n",
    "    ga_1  = (sigmoid(A_1) * (1-sigmoid(A_1)))\n",
    "    print('ga_1 is:\\n', ga_1)\n",
    "    \n",
    "    delta_1 = (np.matmul(delta_2, weights_2)) * ga_1\n",
    "    print('delta_1 is:\\n', delta_1)\n",
    "    newweights_1 = weights_1 + epsilon * data * delta_1.T\n",
    "    \n",
    "    return newweights_1, newweights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807cd01-b9a9-4ac8-b818-541cf81bd8fc",
   "metadata": {},
   "source": [
    "#### Listing 9.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38666f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.13\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1]])\n",
    "print('The data is:', X)\n",
    "W_1 = np.array([[0.5, 0.5], [0.25, 0.25]])\n",
    "W_2 = np.array([[0.25, 0.25], [0.5, 0.5]])\n",
    "Target = np.array([[0.5, 0.5]])\n",
    "epsilon = 0.1\n",
    "\n",
    "Y0,_,_,_ = prediction(X, W_1, W_2)\n",
    "error0 = np.sum((Target - Y0)**2)/2\n",
    "print('The initial error is:', np.round(error0,3))\n",
    "\n",
    "newW_1, newW_2 = update_weights(X, Target, W_1, W_2, epsilon)\n",
    "print('The newW_1 is:\\n', newW_1)\n",
    "print('The newW_2 is:\\n', newW_2)\n",
    "\n",
    "Y, A_2, Z_1, A_1 = prediction(X, newW_1, newW_2)\n",
    "print('The predictions are:\\n', Y)\n",
    "error1 = np.sum((Target - Y)**2)/2\n",
    "print('The error after one iteration is:', np.round(error1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f600a12d",
   "metadata": {},
   "source": [
    "#### Sklearn - MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15477b0e-a14c-4de6-9890-235244c9f9ba",
   "metadata": {},
   "source": [
    "#### Listing 9.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a0a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.14 - Sklearn - MLPRegressor\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train = np.array([[-2], [-1.3], [-1], [0], [1], [1.8], [2]])\n",
    "y_train = np.array([3.2, 1.2, 1.6, 0.1, 0.9, 3.0, 4.2])\n",
    "X_test = np.array([[-1.2], [2.6]])\n",
    "y_test = np.array([1.2, 6.8])\n",
    "\n",
    "mlpreg = MLPRegressor(hidden_layer_sizes=(3, 2), activation = 'relu', solver = 'sgd', max_iter=800, verbose = True, random_state=1)\n",
    "mlpreg.fit(X_train, y_train)\n",
    "biases = [np.round(bias, 2) for bias in mlpreg.intercepts_]\n",
    "print('The trained list of biases is:\\n', biases)\n",
    "weights = [np.round(coef, 2) for coef in mlpreg.coefs_]\n",
    "print('The trained weights are:\\n', weights)\n",
    "\n",
    "y_pred = mlpreg.predict(X_test)\n",
    "print('The predicted values for the test data are:\\n', np.round(y_pred,2))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('The mean squared error on the test set is:\\n', np.round(mse, 3))\n",
    "loss = mlpreg.loss_curve_\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].scatter(X_train, y_train, color='blue', marker='s', label='Training data')\n",
    "ax[0].scatter(X_test, y_test, color='green', marker='x', label='Test data')\n",
    "ax[0].set_xlabel('X')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(loss, label='Training Loss')\n",
    "ax[1].set_xlabel('Iterations')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_and_learning_curve_regression.eps', dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f7de9",
   "metadata": {},
   "source": [
    "#### Sklearn - MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819bcc8-0906-4c2c-a93f-54293196af92",
   "metadata": {},
   "source": [
    "#### Listing 9.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 9.15 - Sklearn - MLPClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = np.random.randn(200, 2)\n",
    "print('the first 5 data:\\n',X[:5])\n",
    "y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n",
    "print('The first five target values:\\n', y[:5])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\n",
    "\n",
    "mlpclf = MLPClassifier(hidden_layer_sizes=(20,), activation = 'relu', solver = 'sgd', max_iter=3000, verbose=True)                   \n",
    "mlpclf.fit(X_train, y_train) \n",
    "B_shapes = [intercept.shape for intercept in mlpclf.intercepts_]\n",
    "print(\"The biases shapes are:\", B_shapes)\n",
    "W_shapes = [weights.shape for weights in mlpclf.coefs_]\n",
    "print('The weights shapes are:', W_shapes)\n",
    "\n",
    "y_pred_prob = mlpclf.predict_proba(X_test)\n",
    "print('The first 5 predicted probabilities:\\n', y_pred_prob[:5])\n",
    "y_pred = mlpclf.predict(X_test)\n",
    "print('The first 5 test data:\\n',X_test[:5])\n",
    "print('The first 5 predictions:\\n', y_pred[:5])\n",
    "print('The overall accuracy of the test data is: \\n', np.round(mlpclf.score(X_test, y_test),2))\n",
    "loss = mlpclf.loss_curve_\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "ax[0].scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], color='r', label='Training: Class 1')\n",
    "ax[0].scatter(X_train[y_train == 0][:, 0], X_train[y_train == 0][:, 1], color='b', label='Training: Class 0')\n",
    "ax[0].scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='r', marker='x', label='Test: Class 1')\n",
    "ax[0].scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='b', marker='x',label='Test: Class 0')\n",
    "ax[0].set_xlabel('X')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].legend(loc='lower left')\n",
    "ax[0].grid(True)\n",
    "\n",
    "ax[1].plot(loss, label='Training Loss')\n",
    "ax[1].set_xlabel('Iterations')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_and_learning_curve_classification.eps', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be041c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bea395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
